üìã Plano de Robustez: Lead Extraction System (Nitro Leads)

Este documento consolida as auditorias realizadas em Janeiro de 2026 e define o roadmap t√©cnico para transformar o sistema em uma plataforma SaaS escal√°vel, segura e lucrativa. Ele serve como guia de contexto para desenvolvimento e refatora√ß√£o via IA.

üöÄ 1. Vis√£o Geral do Roadmap

O projeto deve evoluir de uma arquitetura de "prot√≥tipo" para um sistema de n√≠vel empresarial. A prioridade segue a hierarquia de sobreviv√™ncia do software:

Ordem

Fase

Foco Principal

Impacto

1

Sobreviv√™ncia & Integridade

Seguran√ßa e L√≥gica de Cr√©ditos

Cr√≠tico (Financeiro/Jur√≠dico)

2

Estabilidade Operacional

Infraestrutura e Resili√™ncia

Alto (Disponibilidade)

3

Estrutura & Escalabilidade

Arquitetura e Clean Code

M√©dio (Manuten√ß√£o)

4

Velocidade & UX

Performance de Banco e Mem√≥ria

M√©dio (Experi√™ncia)

üõ°Ô∏è 2. Fase 1: Sobreviv√™ncia e Integridade (Imediato)

Objetivo: Blindar o acesso a dados sens√≠veis e garantir que cada visualiza√ß√£o de dado gere receita.

üìù Itens de A√ß√£o:

Padroniza√ß√£o de Ownership (Risco Cr√≠tico #1): Refatorar todas as views para validar permiss√£o exclusivamente via tabela LeadAccess. Leads "globais" (user=None) n√£o devem ser acess√≠veis sem um v√≠nculo de compra.

Blindagem de Tokens (Risco Cr√≠tico #2): Mover viper_tokens.json para o diret√≥rio /secure/ e configurar Nginx para bloquear qualquer acesso externo a arquivos .json ou √† pasta /secure/.

Corre√ß√£o da L√≥gica de Cobran√ßa: Alterar fun√ß√µes de cache e busca ass√≠ncrona para debitar cr√©ditos sempre que um lead for entregue em uma nova busca, removendo a depend√™ncia de if created.

Sistema de Reembolso (Refund): Implementar a fun√ß√£o refund_credits() no credit_service.py e aplic√°-la em blocos try/except ao redor de chamadas √†s APIs externas (Viper/Serper).

üìë Relat√≥rios de Refer√™ncia (Seguran√ßa e Cr√©ditos)

[Relat√≥rio de Auditoria de Seguran√ßa - Nitro Leads
Resumo executivo
Foram identificados 12 riscos de seguran√ßa, distribu√≠dos em 4 categorias de criticidade. Os principais problemas est√£o em Broken Access Control e Exposi√ß√£o de Dados Sens√≠veis.
üî¥ CR√çTICO
1. Broken Access Control - Valida√ß√£o insuficiente de ownership em search_cpf_batch
Localiza√ß√£o: lead_extractor/views.py:1397-1405
Descri√ß√£o:
A fun√ß√£o search_cpf_batch valida ownership apenas se lead.user existir. Como o modelo Lead permite user=None (campo deprecated), um atacante pode acessar leads globais que n√£o pertencem a nenhum usu√°rio.
views.pyLines 1397-1405
                lead = Lead.objects.filter(id=lead_id).first()                if not lead:                    errors.append(f"Lead {lead_id} n√£o encontrado")                    continue                                # Se o lead tem usu√°rio, validar ownership                if lead.user and lead.user != user_profile:                    errors.append(f"Lead {lead_id} n√£o pertence ao usu√°rio")                    continue
Impacto: Acesso n√£o autorizado a dados de CPFs e s√≥cios de leads globais.
Recomenda√ß√£o: Validar via LeadAccess em vez de lead.user:
# Validar via LeadAccess (modelo correto)lead_access = LeadAccess.objects.filter(lead_id=lead_id, user=user_profile).first()if not lead_access:    errors.append(f"Lead {lead_id} n√£o pertence ao usu√°rio")    continuelead = lead_access.lead
2. Exposi√ß√£o de dados sens√≠veis - viper_tokens.json acess√≠vel via web
Localiza√ß√£o: auth_bot.py:25, lead_extractor/services.py:26
Descri√ß√£o:
O arquivo viper_tokens.json est√° em secure/viper_tokens.json. Se o servidor web servir arquivos fora de STATIC_ROOT ou se houver configura√ß√£o incorreta, o arquivo pode ser acess√≠vel via HTTP.
Impacto: Exposi√ß√£o de tokens de autentica√ß√£o do Viper, permitindo acesso n√£o autorizado √† API.
Recomenda√ß√£o:
Garantir que secure/ esteja fora de qualquer diret√≥rio servido pelo web server
Adicionar .htaccess ou regra nginx para bloquear acesso
Considerar armazenar tokens em vari√°veis de ambiente ou servi√ßo de secrets
3. Exposi√ß√£o de dados sens√≠veis - viper_data com CPFs e s√≥cios em banco sem criptografia
Localiza√ß√£o: lead_extractor/models.py:152
Descri√ß√£o:
O campo viper_data (JSONField) armazena CPFs, dados de s√≥cios e informa√ß√µes sens√≠veis sem criptografia. Se o banco for comprometido, os dados ficam expostos.
Impacto: Viola√ß√£o de LGPD/GDPR, exposi√ß√£o de dados pessoais sens√≠veis.
Recomenda√ß√£o:
Criptografar campos sens√≠veis antes de salvar (ex.: usar django-encrypted-model-fields ou criptografia em n√≠vel de aplica√ß√£o)
Considerar mascarar CPFs parcialmente (ex.: ***.***.***-XX)
Implementar rotina de anonimiza√ß√£o para dados antigos
üü† ALTO
4. Broken Access Control - Valida√ß√£o inconsistente de lead_id em m√∫ltiplas views
Localiza√ß√£o: lead_extractor/views.py:1397, lead_extractor/views.py:1138-1145
Descri√ß√£o:
Algumas views validam ownership via LeadAccess, outras apenas verificam lead.user. Isso cria inconsist√™ncias e poss√≠veis bypasses.
Exemplo correto:
views.pyLines 1138-1142
        lead_accesses_to_enrich = LeadAccess.objects.filter(            lead_id__in=lead_ids,            user=user_profile,            search=search_obj        ).select_related('lead')
Exemplo problem√°tico:
views.pyLines 1397-1405
                lead = Lead.objects.filter(id=lead_id).first()                if not lead:                    errors.append(f"Lead {lead_id} n√£o encontrado")                    continue                                # Se o lead tem usu√°rio, validar ownership                if lead.user and lead.user != user_profile:                    errors.append(f"Lead {lead_id} n√£o pertence ao usu√°rio")                    continue
Recomenda√ß√£o: Padronizar todas as views para usar LeadAccess como √∫nica fonte de verdade para ownership.
5. Inje√ß√£o de comandos - subprocess.Popen com caminho hardcoded no webhook GitHub
Localiza√ß√£o: lead_extractor/views.py:1549-1554
Descri√ß√£o:
O webhook do GitHub executa um script com caminho hardcoded. Embora o caminho seja fixo, n√£o h√° valida√ß√£o adicional do script antes da execu√ß√£o.
views.pyLines 1549-1554
        subprocess.Popen(            [deploy_script],            stdout=subprocess.PIPE,            stderr=subprocess.PIPE,            cwd='/home/nitroleads/apps/nitroleads'        )
Impacto: Se o script for modificado maliciosamente, pode executar comandos arbitr√°rios.
Recomenda√ß√£o:
Adicionar checksum do script e validar antes de executar
Executar com usu√°rio n√£o privilegiado
Adicionar logging detalhado do que foi executado
Considerar usar sistema de filas (Celery) em vez de subprocess direto
6. Autentica√ß√£o - Valida√ß√£o de JWT sem verifica√ß√£o de expira√ß√£o expl√≠cita
Localiza√ß√£o: lead_extractor/middleware.py:68-73
Descri√ß√£o:
O middleware valida o JWT, mas n√£o verifica explicitamente a expira√ß√£o. A biblioteca jose pode fazer isso automaticamente, mas n√£o est√° claro se audience='authenticated' √© suficiente.
middleware.pyLines 68-73
            payload = jwt.decode(                auth_token,                SUPABASE_JWT_SECRET,                algorithms=['HS256'],                audience='authenticated'            )
Recomenda√ß√£o:
Adicionar verifica√ß√£o expl√≠cita de expira√ß√£o: options={"verify_exp": True}
Validar iss (issuer) se aplic√°vel
Implementar refresh token mechanism
7. Exposi√ß√£o de dados sens√≠veis - sanitize_lead_data pode vazar dados em logs
Localiza√ß√£o: lead_extractor/services.py:1260-1289
Descri√ß√£o:
A fun√ß√£o sanitize_lead_data remove dados sens√≠veis antes de enviar ao frontend, mas se houver logging antes da sanitiza√ß√£o, dados sens√≠veis podem aparecer em logs.
Recomenda√ß√£o:
Garantir que logs nunca contenham viper_data completo
Usar m√°scaras em logs (ex.: CPF: ***.***.***-XX)
Revisar todos os pontos de logging que tocam em viper_data
üü° M√âDIO
8. Broken Access Control - Falta valida√ß√£o de ownership em export_leads_csv quando search_id=None
Localiza√ß√£o: lead_extractor/views.py:258-301
Descri√ß√£o:
Quando search_id=None, a fun√ß√£o exporta todos os leads do usu√°rio via LeadAccess, o que est√° correto. Por√©m, se houver leads com user=None compartilhados, pode haver confus√£o.
Impacto: Baixo, pois usa LeadAccess.objects.filter(user=user_profile), mas a l√≥gica pode ser mais clara.
Recomenda√ß√£o: Documentar claramente que apenas leads com LeadAccess do usu√°rio s√£o exportados.
9. Inje√ß√£o de comandos - subprocess.run em run_auth_bot usa vari√°veis de ambiente do processo pai
Localiza√ß√£o: lead_extractor/services.py:131-138
Descri√ß√£o:
O subprocess.run copia todo o ambiente do processo pai (env = os.environ.copy()). Se vari√°veis de ambiente maliciosas forem injetadas, podem afetar o auth_bot.py.
services.pyLines 131-138
        result = subprocess.run(            [sys.executable, str(AUTH_BOT_PATH)],            env=env,            cwd=str(BASE_DIR),            timeout=AUTH_BOT_TIMEOUT,            capture_output=True,            text=True        )
Recomenda√ß√£o:
Criar ambiente m√≠nimo com apenas vari√°veis necess√°rias
Validar valores de vari√°veis cr√≠ticas antes de passar ao subprocess
10. Autentica√ß√£o - Falta rate limiting no middleware de autentica√ß√£o
Localiza√ß√£o: lead_extractor/middleware.py:32-145
Descri√ß√£o:
O SupabaseAuthMiddleware n√£o tem rate limiting. Um atacante pode tentar validar tokens em massa, causando carga no banco.
Recomenda√ß√£o:
Adicionar rate limiting por IP no middleware
Cachear valida√ß√µes de JWT v√°lidos por alguns segundos
Monitorar tentativas de autentica√ß√£o falhadas
üü¢ BAIXO
11. Autentica√ß√£o - Email placeholder tempor√°rio pode ser usado indevidamente
Localiza√ß√£o: lead_extractor/middleware.py:89-91
Descri√ß√£o:
Se o email n√£o estiver no JWT, √© usado um placeholder user_{user_id[:8]}@temp.com. Isso pode causar confus√£o em logs e relat√≥rios.
middleware.pyLines 89-91
                email = f"user_{user_id[:8]}@temp.com"                logger.warning(f"Email n√£o encontrado no JWT para user_id {user_id}, usando placeholder")
Recomenda√ß√£o:
Considerar falhar a autentica√ß√£o se email n√£o estiver presente
Ou buscar email via API do Supabase se n√£o estiver no JWT
12. Exposi√ß√£o de dados sens√≠veis - Diret√≥rio secure/ com permiss√µes 700 pode n√£o ser suficiente
Localiza√ß√£o: auth_bot.py:24
Descri√ß√£o:
O diret√≥rio secure/ √© criado com permiss√µes 700 (apenas owner). Se o processo web rodar com usu√°rio diferente do que cria o diret√≥rio, pode haver problemas.
auth_bot.pyLine 24
SECURE_DIR.mkdir(exist_ok=True, mode=0o700)  # Criar diret√≥rio com permiss√µes restritas (700)
Recomenda√ß√£o:
Garantir que o usu√°rio do processo web tenha acesso ao diret√≥rio
Considerar usar grupo espec√≠fico com permiss√µes adequadas
Documentar requisitos de permiss√µes no README
Estat√≠sticas do relat√≥rio
Total de riscos: 12
Cr√≠ticos: 3
Altos: 4
M√©dios: 3
Baixos: 2
Recomenda√ß√µes priorit√°rias
Imediato: Corrigir valida√ß√£o de ownership em search_cpf_batch (risco #1)
Imediato: Garantir que viper_tokens.json n√£o seja acess√≠vel via web (risco #2)
Curto prazo: Implementar criptografia para viper_data (risco #3)
Curto prazo: Padronizar valida√ß√£o de ownership em todas as views (risco #4)
M√©dio prazo: Revisar e melhorar seguran√ßa do webhook GitHub (risco #5)
Observa√ß√µes positivas
Uso correto de LeadAccess na maioria das views
Uso de subprocess.run com timeout e caminhos absolutos
Valida√ß√£o de JWT com biblioteca confi√°vel (jose)
Uso de transa√ß√µes at√¥micas em credit_service.py
Fun√ß√£o sanitize_lead_data para proteger dados sens√≠veis no frontend
Uso de select_for_update() para prevenir race conditions em cr√©ditos


Relat√≥rio de Conformidade: L√≥gica de Transa√ß√£o de Cr√©ditos
1. Integridade Financeira - Preven√ß√£o de Double Spending
Status: Conforme com ressalvas
A fun√ß√£o debit_credits em credit_service.py implementa prote√ß√µes contra race conditions:
credit_service.pyLines 9-56
def debit_credits(user_id, amount, description=None):    """    Debita cr√©ditos do usu√°rio de forma at√¥mica.    ...    """    try:        with transaction.atomic():            # Se user_id √© um objeto, usar diretamente, sen√£o buscar            if isinstance(user_id, UserProfile):                user_profile = user_id                # Buscar novamente com lock para garantir consist√™ncia                user_profile = UserProfile.objects.select_for_update().get(id=user_profile.id)            else:                user_profile = UserProfile.objects.select_for_update().get(id=user_id)                        # Verificar e debitar cr√©ditos atomicamente usando F() expression            # Isso previne race conditions: a verifica√ß√£o e o d√©bito acontecem em uma √∫nica opera√ß√£o SQL            updated_count = UserProfile.objects.filter(                id=user_profile.id,                credits__gte=amount  # Verifica√ß√£o condicional no banco            ).update(credits=F('credits') - amount)                        if updated_count == 0:                # Atualizar objeto para obter saldo atual                user_profile.refresh_from_db()                return False, user_profile.credits, f"Cr√©ditos insuficientes. Dispon√≠vel: {user_profile.credits}, Necess√°rio: {amount}"                        # Atualizar o objeto para obter o valor atualizado            user_profile.refresh_from_db()                        # Criar transa√ß√£o de uso            CreditTransaction.objects.create(                user=user_profile,                transaction_type='usage',                amount=-amount,  # Negativo para d√©bito                description=description or f"Uso de {amount} cr√©dito(s)"            )                        logger.info(f"Cr√©ditos debitados: {amount} do usu√°rio {user_profile.email}. Novo saldo: {user_profile.credits}")                        return True, user_profile.credits, None                except UserProfile.DoesNotExist:        return False, 0, f"Usu√°rio n√£o encontrado: {user_id}"    except Exception as e:        logger.error(f"Erro ao debitar cr√©ditos: {e}")        return False, 0, str(e)
Prote√ß√µes implementadas:
transaction.atomic() para atomicidade
select_for_update() para lock de linha
Verifica√ß√£o e d√©bito em uma √∫nica opera√ß√£o SQL com F() e filter().update()
Verifica√ß√£o condicional credits__gte=amount no banco
Ressalva:
Se a cria√ß√£o de CreditTransaction falhar ap√≥s o update(), o cr√©dito j√° foi debitado, mas n√£o h√° registro. O rollback do atomic() deveria reverter, mas se houver exce√ß√£o n√£o capturada, pode haver inconsist√™ncia.
2. Regras de Cobran√ßa - "Cobrar para visualizar mesmo se j√° existir no banco"
Status: Parcialmente conforme ‚Äî inconsist√™ncias identificadas
An√°lise por cen√°rio:
2.1. Busca por CNPJ (search_by_cnpj)
Conforme: debita cr√©dito mesmo se o lead j√° existe.
views.pyLines 630-712
        try:            # Verificar se j√° existe Lead com este CNPJ (pode ser de qualquer usu√°rio ou sem usu√°rio)            existing_lead = Lead.objects.filter(cnpj=cnpj_clean).first()                        if existing_lead and existing_lead.viper_data:                # J√° existe - usar dados existentes                logger.info(f"Reutilizando Lead existente {existing_lead.id} para CNPJ {cnpj_clean}")                lead = existing_lead                data = lead.viper_data.copy()                                # Verificar se precisa buscar s√≥cios                if not has_valid_partners_data(lead):                    queue_result = get_partners_internal_queued(cnpj_clean, user_profile, lead=lead)                    queue_id = queue_result.get('queue_id')                    if queue_id:                        partners_data = wait_for_partners_processing(queue_id, user_profile, timeout=60)                        if partners_data:                            data['socios_qsa'] = partners_data                            lead.viper_data = data                            lead.save(update_fields=['viper_data'])                        # Garantir que lead e data est√£o definidos            if not lead or not data:                logger.error(f"Erro: lead ou data n√£o definidos ap√≥s processamento (CNPJ: {cnpj_clean})")                messages.error(request, 'Erro ao processar dados do CNPJ')                return redirect('simple_search')                        # Debitar cr√©dito            success, new_balance, error = debit_credits(                user_profile,                1,                description=f"Busca r√°pida por CNPJ: {cnpj_clean}"            )
2.2. Busca de S√≥cios (search_partners)
Conforme: debita cr√©dito antes de buscar/exibir, mesmo se dados j√° existem.
views.pyLines 1278-1289
                # IMPORTANTE: Debitar cr√©dito ANTES de buscar/exibir s√≥cios                success, new_balance, error = debit_credits(                    user_profile,                    1,                    description=f"S√≥cios (QSA) para {lead.name} (CNPJ: {lead.cnpj})"                )                                if not success:                    errors.append(f"Erro ao debitar cr√©dito para {lead.name}: {error}")                    continue                                credits_debited += 1
2.3. Busca por CPF (search_cpf_batch)
Conforme: debita cr√©dito antes de buscar, mesmo se dados j√° existem.
views.pyLines 1407-1418
                # IMPORTANTE: Debitar cr√©dito ANTES de buscar/exibir dados                success, new_balance, error = debit_credits(                    user_profile,                    1,                    description=f"Busca por CPF: {cpf} ({socio_name})"                )                                if not success:                    errors.append(f"Erro ao debitar cr√©dito para CPF {cpf}: {error}")                    continue                                credits_debited += 1
2.4. Processamento de Busca Ass√≠ncrona (process_search_async)
N√£o conforme: s√≥ debita se LeadAccess for criado (created=True). Se j√° existir, n√£o debita novamente.
services.pyLines 1490-1512
                    # Criar ou obter LeadAccess e debitar cr√©dito                    lead_access, created = LeadAccess.objects.get_or_create(                        user=user_profile,                        lead=lead_obj,                        defaults={                            'search': search_obj,                            'credits_paid': 1,                        }                    )                                        if created:                        success, new_balance, error = debit_credits(                            user_profile,                            1,                            description=f"Lead: {company_data['name']}"                        )                                                # Se d√©bito falhar, PARAR busca completamente                        if not success:                            logger.error(f"D√©bito de cr√©dito falhou: {error}. Parando busca.")                            break                                                credits_used += 1
Problema: se o usu√°rio visualizar o mesmo lead em uma nova busca, n√£o ser√° cobrado novamente se o LeadAccess j√° existir.
2.5. Busca de Leads do Cache (get_leads_from_cache)
N√£o conforme: mesmo problema ‚Äî s√≥ debita se created=True.
services.pyLines 1083-1103
            # Criar LeadAccess e debitar cr√©dito (√© novo acesso)            lead_access, created = LeadAccess.objects.get_or_create(                user=user_profile,                lead=lead,                defaults={                    'search': search_obj,                    'credits_paid': 1,                }            )                        # Se √© novo acesso, debitar cr√©dito            if created:                success, new_balance, error = debit_credits(                    user_profile,                    1,                    description=f"Lead (cache): {lead.name}"                )                                if not success:                    logger.warning(f"Erro ao debitar cr√©dito para lead {lead.id}: {error}")                    # Continuar mesmo se d√©bito falhar (j√° criou LeadAccess)
2.6. Busca de Leads Existentes (get_existing_leads_from_db)
N√£o conforme: mesmo problema.
services.pyLines 972-992
            # Criar LeadAccess e debitar cr√©dito (√© novo acesso)            lead_access, created = LeadAccess.objects.get_or_create(                user=user_profile,                lead=lead,                defaults={                    'search': search_obj,                    'credits_paid': 1,                }            )                        # Se √© novo acesso, debitar cr√©dito            if created:                success, new_balance, error = debit_credits(                    user_profile,                    1,                    description=f"Lead (base existente): {lead.name}"                )                                if not success:                    logger.warning(f"Erro ao debitar cr√©dito para lead {lead.id}: {error}")                    # Continuar mesmo se d√©bito falhar (j√° criou LeadAccess)
Conclus√£o: inconsist√™ncia entre a regra de neg√≥cio e a implementa√ß√£o. Em buscas ass√≠ncronas, o sistema n√£o cobra novamente se o LeadAccess j√° existir, mesmo que o lead seja visualizado em uma nova busca.
3. Fluxo de Reembolso - Falhas na API Externa Ap√≥s D√©bito
Status: N√£o conforme ‚Äî reembolso n√£o implementado
An√°lise:
3.1. Modelo de Dados
O modelo suporta reembolso, mas n√£o h√° implementa√ß√£o:
models.pyLines 116-133
class CreditTransaction(models.Model):    TRANSACTION_TYPES = [        ('purchase', 'Compra'),        ('usage', 'Uso'),        ('refund', 'Reembolso'),    ]    user = models.ForeignKey(UserProfile, on_delete=models.CASCADE, related_name='credit_transactions')    transaction_type = models.CharField(max_length=20, choices=TRANSACTION_TYPES)    amount = models.IntegerField()  # Positivo para compra, negativo para uso    stripe_payment_intent_id = models.CharField(max_length=255, null=True, blank=True)    description = models.TextField(null=True, blank=True)    created_at = models.DateTimeField(auto_now_add=True)    class Meta:        ordering = ['-created_at']    def __str__(self):        return f"{self.get_transaction_type_display()} - {self.amount} cr√©ditos - {self.user.email}"
3.2. Cen√°rios de Falha Sem Reembolso
Cen√°rio 1: Falha na API Viper ap√≥s d√©bito (busca por CNPJ)
views.pyLines 708-730
            # Debitar cr√©dito            success, new_balance, error = debit_credits(                user_profile,                1,                description=f"Busca r√°pida por CNPJ: {cnpj_clean}"            )                        if success:                messages.success(request, 'Busca realizada com sucesso!')                # Garantir que data est√° atualizado no lead                if lead.viper_data != data:                    lead.viper_data = data                    lead.save(update_fields=['viper_data'])                                context = {                    'lead': lead,  # Usar lead real para o template                    'cnpj': cnpj_clean,                    'data': data,  # Usar data diretamente (garantido estar definido)                    'user_profile': user_profile,                    'available_credits': new_balance,                }                return render(request, 'lead_extractor/cnpj_result.html', context)            else:                messages.error(request, f'Erro ao debitar cr√©dito: {error}')
Problema: o d√©bito ocorre antes de validar se os dados da API est√£o completos. Se a API falhar ap√≥s o d√©bito, n√£o h√° reembolso.
Cen√°rio 2: Falha na busca de s√≥cios ap√≥s d√©bito
views.pyLines 1278-1317
                # IMPORTANTE: Debitar cr√©dito ANTES de buscar/exibir s√≥cios                success, new_balance, error = debit_credits(                    user_profile,                    1,                    description=f"S√≥cios (QSA) para {lead.name} (CNPJ: {lead.cnpj})"                )                                if not success:                    errors.append(f"Erro ao debitar cr√©dito para {lead.name}: {error}")                    continue                                credits_debited += 1                                # Verificar se j√° tem s√≥cios salvos no banco (usando fun√ß√£o helper robusta)                has_partners = has_valid_partners_data(lead)                                if has_partners:                    # Dados j√° existem - usar dados salvos (n√£o fazer nova requisi√ß√£o √† API)                    logger.info(f"Usando dados de s√≥cios j√° salvos para Lead {lead.id} (CNPJ: {lead.cnpj}) - n√£o ser√° enfileirado")                else:                    # Dados n√£o existem - buscar via API (mas n√£o aguardar - processar em background)                    if not lead.cnpj:                        errors.append(f"Lead {lead.name} n√£o possui CNPJ")                        continue                                        # Enfileirar busca de s√≥cios (processamento ass√≠ncrono)                    queue_result = get_partners_internal_queued(lead.cnpj, user_profile, lead=lead)                    queue_id = queue_result.get('queue_id')                    is_new = queue_result.get('is_new', True)                                        if not queue_id:                        errors.append(f"Erro ao enfileirar busca de s√≥cios para {lead.name}")                        continue
Problema: se a fila falhar ou a API retornar erro, o cr√©dito j√° foi debitado e n√£o h√° reembolso.
Cen√°rio 3: Falha na busca de CPF ap√≥s d√©bito
views.pyLines 1407-1452
                # IMPORTANTE: Debitar cr√©dito ANTES de buscar/exibir dados                success, new_balance, error = debit_credits(                    user_profile,                    1,                    description=f"Busca por CPF: {cpf} ({socio_name})"                )                                if not success:                    errors.append(f"Erro ao debitar cr√©dito para CPF {cpf}: {error}")                    continue                                credits_debited += 1                                # Verificar se j√° tem dados do CPF salvos no lead                viper_data = lead.viper_data or {}                socios_qsa = viper_data.get('socios_qsa', {})                socios_list = socios_qsa.get('socios', []) if isinstance(socios_qsa, dict) else []                                cpf_clean = cpf.replace('.', '').replace('-', '').strip()                cpf_data = None                found_socio = None                                # Buscar s√≥cio pelo CPF e verificar se j√° tem dados enriquecidos                # NOTA: API Viper retorna CPF no campo 'DOCUMENTO', n√£o 'CPF' ou 'cpf'                for socio in socios_list:                    socio_cpf = str(socio.get('DOCUMENTO') or socio.get('CPF') or socio.get('cpf') or '').replace('.', '').replace('-', '').strip()                    if socio_cpf == cpf_clean:                        found_socio = socio                        # Verificar se j√° tem dados do CPF                        if socio.get('cpf_enriched') and socio.get('cpf_data'):                            # Usar dados salvos (n√£o fazer nova requisi√ß√£o √† API)                            logger.info(f"Usando dados de CPF j√° salvos para {cpf}")                            cpf_data = socio.get('cpf_data')                        break                                if not found_socio:                    errors.append(f"S√≥cio com CPF {cpf} n√£o encontrado no lead {lead_id}")                    continue                                if not cpf_data:                    # Dados n√£o existem - buscar via API                    cpf_data = search_cpf_viper(cpf_clean)                                        if not cpf_data:                        errors.append(f"N√£o foi poss√≠vel obter dados para CPF {cpf}")                        continue
Problema: se search_cpf_viper retornar None, o cr√©dito j√° foi debitado e n√£o h√° reembolso.
3.3. Fun√ß√£o de Reembolso Ausente
N√£o existe fun√ß√£o refund_credits em credit_service.py. O tipo 'refund' existe no modelo, mas n√£o √© utilizado.
Resumo Executivo
Aspecto	Status	Severidade	Observa√ß√µes
Integridade Financeira (Double Spending)	Conforme	Baixa	Prote√ß√µes adequadas com select_for_update() e F() expressions
Regras de Cobran√ßa (Busca CNPJ/CPF/S√≥cios)	Conforme	-	Implementado corretamente
Regras de Cobran√ßa (Busca Ass√≠ncrona)	N√£o Conforme	M√©dia	N√£o cobra novamente se LeadAccess j√° existe
Fluxo de Reembolso	N√£o Conforme	Alta	Sem reembolso quando APIs falham ap√≥s d√©bito
Recomenda√ß√µes Priorit√°rias
Alta prioridade
Implementar fun√ß√£o de reembolso: criar refund_credits() em credit_service.py e utiliz√°-la quando APIs externas falharem ap√≥s o d√©bito.
Revisar l√≥gica de cobran√ßa em buscas ass√≠ncronas: garantir que a visualiza√ß√£o de leads em novas buscas sempre cobre cr√©dito, mesmo se LeadAccess j√° existir (ou ajustar a regra de neg√≥cio se a inten√ß√£o for n√£o cobrar novamente).
M√©dia prioridade
Adicionar rollback de transa√ß√£o: garantir que falhas na cria√ß√£o de CreditTransaction revertam o d√©bito de cr√©ditos.
Implementar retry com reembolso: para APIs externas, implementar retry e, se falhar ap√≥s tentativas, reembolsar.
Baixa prioridade
Adicionar m√©tricas: rastrear d√©bitos sem sucesso correspondente para identificar poss√≠veis perdas de cr√©dito.
Melhorar logging: registrar todos os casos onde cr√©ditos s√£o debitados mas a opera√ß√£o falha.
Conclus√£o
O sistema tem prote√ß√µes contra double spending, mas h√° duas lacunas cr√≠ticas:
Regra de cobran√ßa inconsistente em buscas ass√≠ncronas.
Aus√™ncia de reembolso quando APIs externas falham ap√≥s o d√©bito.
Essas lacunas podem resultar em perda de cr√©ditos para usu√°rios e inconsist√™ncias financeiras. Recomenda-se implementar as corre√ß√µes de alta prioridade antes de escalar o sistema.
]

‚öôÔ∏è 3. Fase 2: Estabilidade Operacional

Objetivo: Impedir quedas do servidor por falta de recursos e tornar as filas de processamento inquebr√°veis.

üìù Itens de A√ß√£o:

Gest√£o de Recursos VPS: Ajustar Gunicorn para 3 workers (para 2 cores CPU) e ativar RotatingFileHandler no LOGGING do Django para evitar que o disco encha.

Resili√™ncia da Fila Viper: Implementar Stale Detection no viper_queue_service.py para resetar itens que fiquem em status processing por mais de 30 minutos.

Timeout e Retry: Adicionar timeouts expl√≠citos em todas as chamadas de subprocesso e implementar retry com backoff exponencial na fila.

üìë Relat√≥rio de Refer√™ncia (SRE & Estabilidade)

[Relat√≥rio de Estabilidade SRE - Sistema de Extra√ß√£o de Leads
1. Resili√™ncia do Bot de Autentica√ß√£o (auth_bot.py)
Pontos √∫nicos de falha identificados
1.1 Depend√™ncia de cron job √∫nico
Problema: Renova√ß√£o de tokens depende de um √∫nico cron job (0 */6 * * *)
Risco: Se o cron falhar silenciosamente, tokens expiram sem renova√ß√£o autom√°tica
Impacto: Alto ‚Äî sistema fica sem autentica√ß√£o ap√≥s 6 horas
1.2 Falta de valida√ß√£o de expira√ß√£o proativa
auth_bot.pyLines 73-183
def refresh_viper_tokens() -> bool:    """    Faz login no Viper e captura tokens de autentica√ß√£o.        Returns:        bool: True se capturou tokens com sucesso, False caso contr√°rio    """
Problema: N√£o h√° verifica√ß√£o de expira√ß√£o antes de usar tokens
Risco: Tokens podem expirar entre renova√ß√µes do cron
Impacto: M√©dio ‚Äî requisi√ß√µes falham at√© detec√ß√£o de 401
1.3 Execu√ß√£o s√≠ncrona do auth_bot durante requisi√ß√µes
services.pyLines 433-500
def get_partners_internal(cnpj, retry=True):    """    Busca o QSA (Quadro de S√≥cios e Administradores) na API interna do Viper.        Fluxo:    1. Tenta ler tokens do arquivo    2. Se n√£o tem tokens e retry=True, executa auth_bot    3. Faz requisi√ß√£o √† API    4. Se receber 401 e retry=True, renova tokens e tenta novamente
Problema: run_auth_bot() roda s√≠ncronamente (at√© 90s) durante requisi√ß√µes
Risco: Timeout de requisi√ß√µes, bloqueio de workers, experi√™ncia ruim
Impacto: Alto ‚Äî degrada√ß√£o de performance
1.4 Race condition no arquivo de tokens
auth_bot.pyLines 32-71
def save_tokens_atomic(data: dict) -> bool:    """    Salva tokens de forma at√¥mica para evitar race conditions.    Escreve em arquivo tempor√°rio e depois renomeia.
Status: Implementado corretamente com write-temp-then-rename
Nota: get_auth_headers() usa file locking, mas h√° risco se m√∫ltiplos processos escreverem simultaneamente
Planos de mitiga√ß√£o
Health check de tokens
Adicionar verifica√ß√£o de expira√ß√£o antes de usar
Renovar proativamente quando pr√≥ximo do vencimento
Sistema de alertas
Monitorar falhas do cron
Alertar quando tokens n√£o forem renovados em 5 horas
Execu√ß√£o ass√≠ncrona
Mover renova√ß√£o de tokens para fila de background
Evitar execu√ß√£o s√≠ncrona durante requisi√ß√µes HTTP
Redund√¢ncia do cron
Adicionar segundo cron com offset (ex: 3h) como backup
Verificar se tokens foram atualizados nas √∫ltimas 6 horas
2. Gerenciamento de Fila (process_viper_queue.py)
An√°lise de locks e deadlocks
2.1 Uso correto de skip_locked=True
viper_queue_service.pyLines 122-150
def process_next_request():    """    Processa o pr√≥ximo item da fila (com lock at√¥mico).    Retorna o objeto processado ou None se n√£o houver itens.        Returns:        ViperRequestQueue: Item processado ou None    """    with transaction.atomic():        # Buscar pr√≥ximo item com lock (skip_locked=True para evitar deadlock)        next_item = ViperRequestQueue.objects.select_for_update(            skip_locked=True        ).filter(            status='pending'        ).order_by(            '-priority',  # Maior prioridade primeiro            'created_at'  # Mais antigo primeiro dentro da mesma prioridade        ).first()
Status: Implementado corretamente ‚Äî skip_locked=True evita deadlocks
Nota: M√∫ltiplos workers podem rodar sem conflito
2.2 Risco de itens presos em "processing"
Problema: Se o worker morrer durante processamento, item fica em processing indefinidamente
Impacto: Alto ‚Äî itens ficam presos, fila para de processar
2.3 Falta de timeout de processamento
process_viper_queue.pyLines 98-192
        while True:            try:                # Buscar pr√≥ximo item da fila (com lock)                queue_item = process_next_request()                                if queue_item:                    self.stdout.write(f'Processando requisi√ß√£o {queue_item.id} (tipo: {queue_item.request_type})...')                                        try:                        # Processar baseado no tipo de requisi√ß√£o                        if queue_item.request_type == 'partners':                            cnpj = queue_item.request_data.get('cnpj')                            if not cnpj:                                raise ValueError('CNPJ n√£o encontrado nos dados da requisi√ß√£o')                                                        # Chamar fun√ß√£o original (sem retry aqui, pois j√° est√° na fila)                            result = get_partners_internal(cnpj, retry=True)
Problema: N√£o h√° timeout m√°ximo por item
Risco: Requisi√ß√£o pode travar indefinidamente
Impacto: M√©dio ‚Äî worker fica bloqueado
2.4 Falta de retry autom√°tico para falhas
process_viper_queue.pyLines 160-164
                    except Exception as e:                        error_msg = str(e)                        mark_request_failed(queue_item, error_msg)                        self.stdout.write(self.style.ERROR(f'‚úó Requisi√ß√£o {queue_item.id} falhou: {error_msg}'))                        logger.error(f"Erro ao processar requisi√ß√£o {queue_item.id}: {error_msg}", exc_info=True)
Problema: Falhas s√£o marcadas como failed sem retry
Risco: Falhas transit√≥rias (rede, timeout) n√£o s√£o recuperadas
Impacto: M√©dio ‚Äî perda de requisi√ß√µes v√°lidas
Planos de mitiga√ß√£o
Stale detection
Detectar itens em processing h√° mais de X minutos (ex: 30)
Resetar para pending automaticamente
Timeout por requisi√ß√£o
Adicionar timeout m√°ximo (ex: 5 minutos)
Marcar como failed se exceder
Retry com backoff
Adicionar campo retry_count ao modelo
Retry autom√°tico at√© N tentativas com backoff exponencial
Heartbeat do worker
Worker atualiza timestamp periodicamente
Detectar workers mortos e resetar seus itens
3. Infraestrutura (Supervisor e Nginx)
3.1 Configura√ß√£o do Supervisor
Gunicorn (nitroleads.conf)
nitroleads.confLines 9-40
[program:nitroleads]# Comando para executar o Gunicorn# Workers: 2n+1 onde n = n√∫mero de CPUs. Para 2 cores: 5 workers (otimizado para performance)command=/home/nitroleads/apps/nitroleads/venv/bin/gunicorn lead_extraction.wsgi:application --bind 127.0.0.1:8000 --workers 5 --timeout 120 --threads 2
Problema: 5 workers para 2 cores pode ser excessivo
Risco: Sobrecarga de mem√≥ria em servidor compartilhado (Hostinger)
Impacto: M√©dio ‚Äî poss√≠vel OOM kill
Processador de fila (nitroleads-queue.conf)
nitroleads-queue.confLines 13-59
[program:nitroleads-queue]# Comando para executar o processador de filacommand=/home/nitroleads/apps/nitroleads/venv/bin/python /home/nitroleads/apps/nitroleads/manage.py process_viper_queue# Diret√≥rio de trabalho (IMPORTANTE: auth_bot.py e viper_tokens.json est√£o aqui)directory=/home/nitroleads/apps/nitroleads# Usu√°rio que executa o processouser=nitroleads# Iniciar automaticamente quando o supervisor iniciarautostart=true# Reiniciar automaticamente se o processo morrerautorestart=true# Arquivos de logstderr_logfile=/home/nitroleads/logs/nitroleads/viper_queue_error.logstdout_logfile=/home/nitroleads/logs/nitroleads/viper_queue.log# Tamanho m√°ximo dos arquivos de log (10MB)stderr_logfile_maxbytes=10MBstdout_logfile_maxbytes=10MB# Manter 5 backups dos logsstderr_logfile_backups=5stdout_logfile_backups=5# Tempo m√°ximo de espera ao parar o processo (10 minutos para processar requisi√ß√£o atual)stopwaitsecs=600# Matar todo o grupo de processos ao pararkillasgroup=true# Prioridade (menor n√∫mero = inicia primeiro)priority=998
Problema: stopwaitsecs=600 (10 minutos) pode ser longo
Risco: Rein√≠cio lento do servi√ßo
Impacto: Baixo ‚Äî apenas em manuten√ß√£o
Problema: Falta startretries e startsecs
Risco: Rein√≠cio excessivo se houver falha inicial
Impacto: M√©dio ‚Äî loop de rein√≠cio
3.2 Configura√ß√£o do Nginx
nginx-security-rules.confLines 1-32
# Regras de Seguran√ßa para Nginx - NitroLeads# Adicionar estas regras ao arquivo de configura√ß√£o do Nginx# Bloquear acesso ao diret√≥rio secure/ e arquivos de tokenslocation ~ ^/(secure|viper_tokens\.json) {    deny all;    return 404;}
Status: Regras de seguran√ßa adequadas
Nota: Verificar se est√£o aplicadas no nginx principal
Planos de mitiga√ß√£o
Ajustar workers do Gunicorn
Reduzir para 3 workers (2 cores)
Monitorar uso de mem√≥ria
Adicionar limites de recursos
stopasgroup=true e killasgroup=true j√° presentes
Adicionar startretries=3 e startsecs=10
Timeout do Gunicorn
--timeout 120 pode ser curto para requisi√ß√µes longas
Avaliar aumentar para 180s ou adicionar endpoint de health check
4. Logs e Monitoramento
4.1 Configura√ß√£o atual de logs
Django logging
settings.pyLines 176-217
# Logging ConfigurationLOGGING = {    'version': 1,    'disable_existing_loggers': False,    'formatters': {        'verbose': {            'format': '{levelname} {asctime} {module} {message}',            'style': '{',        },        'simple': {            'format': '{levelname} {message}',            'style': '{',        },    },    'handlers': {        'console': {            'class': 'logging.StreamHandler',            'formatter': 'verbose',        },        'file': {            'class': 'logging.FileHandler',            'filename': BASE_DIR / 'logs' / 'django.log',            'formatter': 'verbose',        },    },    'root': {        'handlers': ['console'],        'level': 'INFO',    },    'loggers': {        'django': {            'handlers': ['console', 'file'],            'level': config('DJANGO_LOG_LEVEL', default='INFO'),            'propagate': False,        },        'lead_extractor': {            'handlers': ['console', 'file'],            'level': 'INFO',            'propagate': False,        },    },}
Problemas identificados
Logs sem rota√ß√£o autom√°tica
django.log pode crescer indefinidamente
Risco: Disco cheio, performance degradada
Impacto: Alto
Falta de logs estruturados
Formato texto dificulta an√°lise
Sem correla√ß√£o de requisi√ß√µes (request_id)
Impacto: M√©dio ‚Äî depura√ß√£o dif√≠cil
Logs do auth_bot separados
Cron redireciona para /home/nitroleads/logs/nitroleads/auth_bot.log
N√£o integrado com logs do Django
Impacto: M√©dio ‚Äî visibilidade fragmentada
Falta de m√©tricas
Sem m√©tricas de fila (tamanho, tempo de processamento)
Sem alertas de falhas
Impacto: Alto ‚Äî problemas n√£o detectados
Logs do Supervisor limitados
Rota√ß√£o configurada (10MB, 5 backups)
Sem integra√ß√£o com sistema centralizado
Impacto: Baixo ‚Äî aceit√°vel para ambiente pequeno
Planos de mitiga√ß√£o
Rota√ß√£o de logs do Django
Usar RotatingFileHandler ou TimedRotatingFileHandler
Limitar tamanho e manter backups
Logs estruturados
Adicionar request_id em todas as requisi√ß√µes
Considerar JSON logging para an√°lise
Health checks e m√©tricas
Endpoint /health com status de componentes
M√©tricas b√°sicas: tamanho da fila, taxa de sucesso/falha
Dashboard simples ou integra√ß√£o com ferramentas
Alertas cr√≠ticos
Falhas consecutivas do auth_bot
Fila com mais de X itens pendentes
Itens presos em processing
Log aggregation
Centralizar logs do Django, Supervisor e cron
Ferramenta simples (ex: multitail) ou servi√ßo externo
5. Cen√°rios de falha previstos
Cen√°rio 1: Token expira durante madrugada
Causa: Cron falha ou token expira antes da renova√ß√£o
Sintomas: Requisi√ß√µes retornam 401, fila para de processar
Detec√ß√£o: Logs de erro 401, fila cresce
Mitiga√ß√£o: Health check de tokens, alertas, retry autom√°tico
Cen√°rio 2: Worker morre durante processamento
Causa: OOM kill, crash, rein√≠cio do servidor
Sintomas: Itens presos em processing, fila para
Detec√ß√£o: Stale detection, monitoramento de workers
Mitiga√ß√£o: Auto-recovery, reset de itens presos
Cen√°rio 3: Fila cresce al√©m da capacidade
Causa: Pico de requisi√ß√µes ou processamento lento
Sintomas: Tempo de resposta alto, timeouts
Detec√ß√£o: M√©tricas de tamanho da fila
Mitiga√ß√£o: Scaling horizontal, prioriza√ß√£o, rate limiting
Cen√°rio 4: Disco cheio por logs
Causa: Logs sem rota√ß√£o crescem indefinidamente
Sintomas: Aplica√ß√£o para de escrever logs, poss√≠vel falha
Detec√ß√£o: Monitoramento de espa√ßo em disco
Mitiga√ß√£o: Rota√ß√£o autom√°tica, limpeza de logs antigos
Cen√°rio 5: M√∫ltiplas execu√ß√µes do auth_bot simult√¢neas
Causa: Cron e requisi√ß√£o HTTP executam ao mesmo tempo
Sintomas: Race condition, poss√≠vel corrup√ß√£o de tokens
Detec√ß√£o: Logs de conflito
Mitiga√ß√£o: Lock distribu√≠do ou fila √∫nica para renova√ß√£o
6. Resumo executivo
Prioridade alta
Stale detection para itens presos em processing
Rota√ß√£o autom√°tica de logs do Django
Health check e alertas de tokens
Retry autom√°tico com backoff para falhas transit√≥rias
Prioridade m√©dia
Reduzir workers do Gunicorn (3 em vez de 5)
Timeout m√°ximo por item da fila
Logs estruturados com request_id
M√©tricas b√°sicas de fila
Prioridade baixa
Redund√¢ncia do cron (backup com offset)
Dashboard de monitoramento
Log aggregation centralizado
Ajustes finos de timeouts
7. Recomenda√ß√µes imediatas
Implementar stale detection (1-2 horas)
Adicionar rota√ß√£o de logs (30 minutos)
Criar health check endpoint (1 hora)
Reduzir workers do Gunicorn (5 minutos)
Adicionar alertas b√°sicos (2-3 horas)
Este relat√≥rio identifica os principais riscos de estabilidade. Recomendo come√ßar pelas a√ß√µes de prioridade alta para melhorar a resili√™ncia do sistema.]

üèóÔ∏è 4. Fase 3: Estrutura e Escalabilidade

Objetivo: Organizar o c√≥digo para permitir a entrada de novas APIs e facilitar a manuten√ß√£o.

üìù Itens de A√ß√£o:

Modulariza√ß√£o (Desmembramento): Dividir views.py e services.py (atualmente com +1500 linhas) em pastas e m√≥dulos por dom√≠nio (Ex: services/lead_service.py, services/search_service.py).

Abstra√ß√£o de Busca (Providers): Implementar a interface SearchProvider (Classe Abstrata) para desacoplar o sistema da API espec√≠fica do Serper.

Camada de Servi√ßo (Services): Remover regras de neg√≥cio, valida√ß√µes de CPF e c√°lculos de cr√©ditos das Views, centralizando-as em m√©todos de Service test√°veis.

üìë Relat√≥rio de Refer√™ncia (Arquitetura)

[Relat√≥rio de revis√£o arquitetural ‚Äî Lead Extraction SaaS
1. Acoplamento: Views vs Services
Problemas identificados
1.1 Regras de neg√≥cio nas views
has_valid_partners_data() em views.py (linhas 28-65): l√≥gica de valida√ß√£o que deveria estar em services.py ou em um m√©todo do modelo.
Normaliza√ß√£o de dados de CPF em search_by_cpf() (linhas 432-532): l√≥gica de normaliza√ß√£o misturada com apresenta√ß√£o.
L√≥gica de enriquecimento em enrich_leads() (linhas 1108-1223): valida√ß√µes e processamento que deveriam estar em services.
1.2 Views fazendo queries diretas
dashboard() (linha 232): queries diretas no modelo.
export_leads_csv() (linha 288): queries complexas na view.
search_history() (linha 775): queries com prefetch na view.
1.3 Processamento ass√≠ncrono na view
dashboard() (linha 217): cria√ß√£o de thread diretamente na view. Deveria estar em um service ou task queue.
Recomenda√ß√µes
Mover has_valid_partners_data() para services.py ou criar m√©todo no modelo Lead.
Criar LeadService para centralizar opera√ß√µes de leads.
Extrair l√≥gica de normaliza√ß√£o de CPF para services.py.
Usar Celery ou Django-Q para processamento ass√≠ncrono.
2. DRY (Don't Repeat Yourself)
Duplica√ß√µes identificadas
2.1 Busca e enriquecimento de leads
Padr√£o repetido em m√∫ltiplos lugares:
# Padr√£o repetido em: process_search_async, enrich_leads, search_partnerscnpj = find_cnpj_by_name(company_name)public_data = enrich_company_viper(cnpj)if public_data:    company_data['viper_data'].update(public_data)
Ocorr√™ncias:
process_search_async() (linhas 1450-1465)
enrich_leads() (linhas 1173-1195)
search_incremental() (linhas 1235-1242)
2.2 Cria√ß√£o/atualiza√ß√£o de Lead
Padr√£o repetido:
existing_lead = Lead.objects.filter(cnpj=cnpj).first()if existing_lead:    lead_obj = existing_lead    # Atualizar dados...else:    lead_obj = Lead.objects.create(...)
Ocorr√™ncias:
process_search_async() (linhas 1468-1488)
search_incremental() (linhas 1657-1677)
2.3 Sanitiza√ß√£o de dados
Padr√£o repetido:
sanitized_viper_data = sanitize_lead_data(    {'viper_data': lead.viper_data or {}},    show_partners=(lead_access.enriched_at is not None)).get('viper_data', {})
Ocorr√™ncias:
get_existing_leads_from_db() (linhas 995-998)
get_leads_from_cache() (linhas 1106-1109)
process_search_async() (linhas 1515-1518)
2.4 Valida√ß√£o de cr√©ditos
Padr√£o repetido:
available_credits = check_credits(user_profile)if available_credits < quantity:    return JsonResponse({'error': 'Cr√©ditos insuficientes'}, status=402)
Ocorr√™ncias:
search_by_cpf() (linha 420)
search_by_cnpj() (linha 625)
enrich_leads() (linha 1131)
search_partners() (linha 1253)
2.5 Formata√ß√£o de dados de CPF
L√≥gica de normaliza√ß√£o duplicada em search_by_cpf() (linhas 432-532) e possivelmente em outros lugares.
Recomenda√ß√µes
Criar LeadEnrichmentService com m√©todos:
enrich_lead_by_cnpj(cnpj) ‚Üí retorna dados enriquecidos
get_or_create_lead(place_data, cnpj) ‚Üí cria/atualiza lead
format_lead_for_response(lead, user_profile) ‚Üí sanitiza e formata
Criar CreditValidationService:
validate_credits(user_profile, required_amount) ‚Üí valida e retorna erro padronizado
Extrair normaliza√ß√£o de CPF para services.py:
normalize_cpf_response(cpf_data) ‚Üí normaliza resposta da API
3. Padroniza√ß√£o Django 4.2
Pontos positivos
Uso de JSONField para dados flex√≠veis
select_related e prefetch_related em alguns lugares
Transa√ß√µes at√¥micas em credit_service.py
Middleware customizado bem estruturado
Decorators reutiliz√°veis (require_user_profile, validate_user_ownership)
Problemas identificados
3.1 Falta de Class-Based Views (CBV)
Todas as views s√£o function-based. Django 4.2 recomenda CBVs para:
Reutiliza√ß√£o de c√≥digo
Mixins para funcionalidades comuns
Melhor organiza√ß√£o
Exemplo:
# Atual (function-based)def dashboard(request):    ...# Recomendado (CBV)class DashboardView(LoginRequiredMixin, TemplateView):    template_name = 'lead_extractor/dashboard.html'        def post(self, request, *args, **kwargs):        ...
3.2 Falta de Forms
Valida√ß√£o manual em vez de Django Forms:
# Atual (valida√ß√£o manual)niche = request.POST.get('niche', '').strip()if not niche or not location:    messages.error(request, 'Por favor, preencha o nicho e a localiza√ß√£o.')
Deveria usar:
class SearchForm(forms.Form):    niche = forms.CharField(max_length=255)    location = forms.CharField(max_length=255)    quantity = forms.IntegerField(min_value=1, max_value=1000)
3.3 Falta de Managers customizados
Queries repetidas que poderiam estar em Managers:
# Em models.pyclass LeadManager(models.Manager):    def with_cnpj(self):        return self.exclude(cnpj__isnull=True).exclude(cnpj='')        def by_cached_search(self, cached_search):        return self.filter(cached_search=cached_search).with_cnpj()
3.4 Falta de Signals
L√≥gica p√≥s-cria√ß√£o que poderia usar Signals:
# Em models.py ou signals.py@receiver(post_save, sender=Lead)def update_cached_search_count(sender, instance, created, **kwargs):    if created and instance.cached_search:        # Atualizar contador...
3.5 Falta de API Views estruturadas
Endpoints JSON poderiam usar Django REST Framework ou pelo menos views mais estruturadas.
Recomenda√ß√µes
Migrar views principais para CBVs (gradualmente)
Criar Forms para valida√ß√£o de entrada
Adicionar Managers customizados para queries comuns
Usar Signals para l√≥gica p√≥s-cria√ß√£o/atualiza√ß√£o
Considerar Django REST Framework para APIs
4. Manutenibilidade: adicionar nova API de busca
Dificuldades atuais
4.1 L√≥gica espalhada
Para adicionar uma nova API (ex: Bing Places), seria necess√°rio:
Adicionar fun√ß√£o em services.py (ex: search_bing_places())
Modificar process_search_async() para incluir nova API
Modificar search_google_hybrid() ou criar fun√ß√£o similar
Atualizar normaliza√ß√£o em normalize_places_response()
Atualizar m√∫ltiplas views que usam busca
4.2 Falta de abstra√ß√£o
N√£o h√° interface/classe base para APIs de busca. Cada API tem sua pr√≥pria implementa√ß√£o.
4.3 Acoplamento forte
process_search_async() est√° fortemente acoplado ao Serper/Google Maps.
Estrutura recomendada
4.1 Criar interface base para APIs
# lead_extractor/services/search_providers.pyfrom abc import ABC, abstractmethodclass SearchProvider(ABC):    @abstractmethod    def search(self, query: str, num: int = 10, start: int = 0) -> list:        """Busca lugares e retorna lista normalizada"""        pass        @abstractmethod    def normalize_response(self, response_data: dict) -> list:        """Normaliza resposta da API para formato padr√£o"""        passclass SerperSearchProvider(SearchProvider):    def search(self, query: str, num: int = 10, start: int = 0) -> list:        # Implementa√ß√£o atual de search_google_maps        passclass BingPlacesProvider(SearchProvider):    def search(self, query: str, num: int = 10, start: int = 0) -> list:        # Nova implementa√ß√£o        pass
4.2 Service de busca unificado
# lead_extractor/services/search_service.pyclass SearchService:    def __init__(self, providers: list[SearchProvider]):        self.providers = providers        def search(self, query: str, max_results: int) -> list:        """Busca usando m√∫ltiplos provedores"""        all_results = []        for provider in self.providers:            results = provider.search(query, num=max_results)            all_results.extend(results)            if len(all_results) >= max_results:                break        return all_results[:max_results]
4.3 Factory pattern para provedores
# lead_extractor/services/search_factory.pydef get_search_providers() -> list[SearchProvider]:    """Retorna lista de provedores configurados"""    providers = []        if config('SERPER_API_KEY'):        providers.append(SerperSearchProvider())        if config('BING_API_KEY'):        providers.append(BingPlacesProvider())        return providers
Benef√≠cios
Adicionar nova API: criar classe que implementa SearchProvider
Configura√ß√£o centralizada via factory
Testes isolados por provedor
F√°cil trocar ordem/prioridade de provedores
5. Estrutura de pastas
Estrutura atual
lead_extractor/‚îú‚îÄ‚îÄ services.py (1761 linhas!) ‚ùå‚îú‚îÄ‚îÄ views.py (1564 linhas!) ‚ùå‚îú‚îÄ‚îÄ models.py‚îú‚îÄ‚îÄ credit_service.py‚îú‚îÄ‚îÄ stripe_service.py‚îú‚îÄ‚îÄ viper_queue_service.py‚îî‚îÄ‚îÄ decorators.py
Problemas
Arquivos muito grandes (services.py com 1761 linhas, views.py com 1564 linhas)
Falta de organiza√ß√£o por dom√≠nio
Tudo misturado em poucos arquivos
Estrutura recomendada
lead_extractor/‚îú‚îÄ‚îÄ models/‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ   ‚îú‚îÄ‚îÄ user.py‚îÇ   ‚îú‚îÄ‚îÄ lead.py‚îÇ   ‚îú‚îÄ‚îÄ search.py‚îÇ   ‚îî‚îÄ‚îÄ credit.py‚îú‚îÄ‚îÄ services/‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ   ‚îú‚îÄ‚îÄ search/‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_service.py‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_providers.py‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ search_factory.py‚îÇ   ‚îú‚îÄ‚îÄ enrichment/‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lead_enrichment.py‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ viper_enrichment.py‚îÇ   ‚îú‚îÄ‚îÄ credit/‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ credit_service.py (mover de credit_service.py)‚îÇ   ‚îî‚îÄ‚îÄ payment/‚îÇ       ‚îú‚îÄ‚îÄ __init__.py‚îÇ       ‚îî‚îÄ‚îÄ stripe_service.py (mover de stripe_service.py)‚îú‚îÄ‚îÄ views/‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ   ‚îú‚îÄ‚îÄ auth.py‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py‚îÇ   ‚îú‚îÄ‚îÄ search.py‚îÇ   ‚îú‚îÄ‚îÄ lead.py‚îÇ   ‚îî‚îÄ‚îÄ payment.py‚îú‚îÄ‚îÄ api/‚îÇ   ‚îú‚îÄ‚îÄ __init__.py‚îÇ   ‚îú‚îÄ‚îÄ views.py (endpoints JSON)‚îÇ   ‚îî‚îÄ‚îÄ serializers.py (se usar DRF)‚îî‚îÄ‚îÄ utils/    ‚îú‚îÄ‚îÄ __init__.py    ‚îú‚îÄ‚îÄ decorators.py    ‚îî‚îÄ‚îÄ validators.py
Resumo executivo
Prioridade alta
Extrair regras de neg√≥cio das views para services
Criar LeadService e LeadEnrichmentService para eliminar duplica√ß√£o
Refatorar process_search_async() para usar abstra√ß√£o de provedores de busca
Dividir services.py e views.py em m√≥dulos menores
Prioridade m√©dia
Migrar para Class-Based Views gradualmente
Criar Forms para valida√ß√£o
Adicionar Managers customizados nos models
Implementar interface SearchProvider para novas APIs
Prioridade baixa
Considerar Django REST Framework para APIs
Usar Signals para l√≥gica p√≥s-cria√ß√£o
Reorganizar estrutura de pastas por dom√≠nio
M√©tricas de qualidade
M√©trica	Atual	Recomendado
Linhas por arquivo	1761 (services.py)	< 500
Acoplamento Views-Services	Alto	Baixo
Duplica√ß√£o de c√≥digo	~30%	< 5%
Facilidade de adicionar API	Dif√≠cil	F√°cil
Testabilidade	M√©dia	Alta
Conclus√£o
O projeto funciona, mas precisa de refatora√ß√£o arquitetural para escalar. Os principais problemas s√£o:
Acoplamento forte entre views e l√≥gica de neg√≥cio
Duplica√ß√£o significativa de c√≥digo
Falta de abstra√ß√£o para adicionar novas APIs
Arquivos muito grandes dificultando manuten√ß√£o
Com as melhorias sugeridas, o c√≥digo ficar√° mais:
Manuten√≠vel: f√°cil de entender e modificar
Test√°vel: l√≥gica isolada e test√°vel
Escal√°vel: f√°cil adicionar novas funcionalidades
Padronizado: segue pr√°ticas do Django 4.2]

‚ö° 5. Fase 4: Velocidade e UX

Objetivo: Reduzir o tempo de carregamento das p√°ginas e o consumo de mem√≥ria do servidor.

üìù Itens de A√ß√£o:

Otimiza√ß√£o SQL: Aplicar .defer('viper_data') em todas as consultas de listagem (Dashboard/Hist√≥rico) para n√£o carregar JSONs pesados desnecessariamente.

√çndices de Banco: Criar √≠ndices parciais no PostgreSQL para filtrar CNPJs v√°lidos e √≠ndices GIN para o campo request_data na fila.

Sanitiza√ß√£o Eficiente: Refatorar sanitize_lead_data para evitar copy.deepcopy(), utilizando constru√ß√£o seletiva de dicion√°rios.

üìë Relat√≥rio de Refer√™ncia (Performance & DB)

[Relat√≥rio de Performance e Otimiza√ß√£o de Banco de Dados
1. Gargalos de Query (N+1)
Problemas identificados
1.1 Carregamento completo de viper_data em loops
Localiza√ß√£o: services.py - fun√ß√µes get_leads_from_cache() e get_existing_leads_from_db()
Problema:
services.pyLines 1072-1120
        # Processar leads que o usu√°rio ainda n√£o acessou        for lead in cached_leads_new:            if len(results) >= quantity:                break                            cnpj = lead.cnpj                        # Evitar duplicatas na mesma busca            if cnpj in cnpjs_processed:                continue            cnpjs_processed.add(cnpj)                        # Criar LeadAccess e debitar cr√©dito (√© novo acesso)            lead_access, created = LeadAccess.objects.get_or_create(                user=user_profile,                lead=lead,                defaults={                    'search': search_obj,                    'credits_paid': 1,                }            )                        # Se √© novo acesso, debitar cr√©dito            if created:                success, new_balance, error = debit_credits(                    user_profile,                    1,                    description=f"Lead (cache): {lead.name}"                )                                if not success:                    logger.warning(f"Erro ao debitar cr√©dito para lead {lead.id}: {error}")                    # Continuar mesmo se d√©bito falhar (j√° criou LeadAccess)                        # Sanitizar dados (esconder QSA/telefones at√© enriquecer)            sanitized_viper_data = sanitize_lead_data(                {'viper_data': lead.viper_data or {}},                show_partners=(lead_access.enriched_at is not None)            ).get('viper_data', {})
Impacto: cada itera√ß√£o carrega o JSON completo de viper_data (pode ter centenas de KB), mesmo quando s√≥ campos b√°sicos s√£o usados.
1.2 Consulta N+1 em export_leads_csv
Localiza√ß√£o: views.py:288
Problema:
views.pyLines 288-303
    # Buscar leads via LeadAccess (garantindo ownership)    # Usar select_related para evitar N+1 queries    lead_accesses = LeadAccess.objects.filter(user=user_profile).select_related('lead', 'search', 'lead__cached_search').order_by('-accessed_at')        # Se search_id fornecido, filtrar por pesquisa (j√° validado acima)    is_last_search = False    if search_id:        lead_accesses = lead_accesses.filter(search=search_obj)                # Verificar se √© a √∫ltima pesquisa (mais recente)        last_search = Search.objects.filter(user=user_profile).order_by('-created_at').first()        if last_search and last_search.id == search_id:            is_last_search = True    # Contar leads para log de auditoria    leads_count = lead_accesses.count()    for lead_access in lead_accesses:
Impacto: select_related est√° presente, mas lead.viper_data √© carregado completo para todos os registros, mesmo que s√≥ alguns campos sejam usados.
1.3 M√∫ltiplas consultas em get_existing_leads_from_db
Localiza√ß√£o: services.py:936-948
Problema:
services.pyLines 936-948
        # Buscar CNPJs que o usu√°rio j√° tem acesso nas 3 √∫ltimas pesquisas        last_3_searches = Search.objects.filter(            user=user_profile        ).order_by('-created_at')[:3]                accessed_cnpjs = set()        if last_3_searches.exists():            last_3_search_ids = set(last_3_searches.values_list('id', flat=True))            accessed_cnpjs = set(                LeadAccess.objects.filter(                    user=user_profile,                    search_id__in=last_3_search_ids                ).values_list('lead__cnpj', flat=True)            )
Impacto: 3 queries separadas (Search, values_list, LeadAccess) que poderiam ser reduzidas.
Recomenda√ß√µes
Usar .defer('viper_data') ou .only() em queries que n√£o precisam do JSON completo:
# Em get_leads_from_cache e get_existing_leads_from_dbcached_leads_new = Lead.objects.filter(    cached_search=cached_search,    cnpj__isnull=False).exclude(cnpj='').defer('viper_data').order_by('-created_at')[:quantity * 3]# Carregar viper_data apenas quando necess√°rio (lazy loading)# Ou usar .only('id', 'name', 'address', 'phone_maps', 'cnpj', 'cached_search_id')
Otimizar get_existing_leads_from_db com uma √∫nica query:
# Substituir 3 queries por 1 usando Subqueryfrom django.db.models import OuterRef, Subqueryaccessed_cnpjs = set(    LeadAccess.objects.filter(        user=user_profile,        search__in=Search.objects.filter(            user=user_profile        ).order_by('-created_at')[:3]    ).values_list('lead__cnpj', flat=True).distinct())
Adicionar prefetch_related em search_history:
# views.py:775searches = Search.objects.filter(user=user_profile).select_related(    'user', 'cached_search').prefetch_related(    'lead_accesses__lead'  # J√° existe, mas pode melhorar).only(    'id', 'niche', 'location', 'created_at', 'status', 'results_count').order_by('-created_at')[:3]
2. Estrat√©gia de Indexa√ß√£o
√çndices faltantes
2.1 LeadAccess - busca por usu√°rio e data
Localiza√ß√£o: models.py:182-186
Problema:
models.pyLines 182-186
        indexes = [            models.Index(fields=['user', 'accessed_at']),            models.Index(fields=['lead', 'user']),            models.Index(fields=['search', 'user']),        ]
An√°lise: o √≠ndice ['user', 'accessed_at'] existe, mas a ordem pode n√£o ser ideal para order_by('-accessed_at'). PostgreSQL pode n√£o usar o √≠ndice de forma eficiente em ordena√ß√£o descendente.
Recomenda√ß√£o:
# Adicionar √≠ndice funcional para ordena√ß√£o descendente# Ou criar √≠ndice composto otimizadoindexes = [    models.Index(fields=['user', '-accessed_at']),  # PostgreSQL suporta DESC    models.Index(fields=['lead', 'user']),    models.Index(fields=['search', 'user']),    # Novo: √≠ndice para buscas frequentes de CNPJs acessados    models.Index(fields=['user', 'lead__cnpj'], name='leadaccess_user_cnpj_idx'),]
2.2 Lead - busca por CNPJ e cached_search
Localiza√ß√£o: models.py:159-162
Problema:
models.pyLines 159-162
        indexes = [            models.Index(fields=['cnpj']),            models.Index(fields=['cached_search', 'cnpj']),  # Para get_leads_from_cache otimizado        ]
An√°lise: o √≠ndice composto ['cached_search', 'cnpj'] √© √∫til, mas falta √≠ndice para filtros com cnpj__isnull=False e exclude(cnpj='').
Recomenda√ß√µes:
# Adicionar √≠ndices parciais (PostgreSQL)indexes = [    models.Index(fields=['cnpj']),    models.Index(fields=['cached_search', 'cnpj']),    # √çndice parcial para CNPJs v√°lidos (reduz tamanho do √≠ndice)    models.Index(        fields=['cached_search', '-created_at'],        condition=Q(cnpj__isnull=False) & ~Q(cnpj=''),        name='lead_cached_search_cnpj_valid_idx'    ),]
2.3 ViperRequestQueue - busca em JSONField
Localiza√ß√£o: models.py:222-227
Problema:
models.pyLines 222-227
        indexes = [            models.Index(fields=['status', 'priority', 'created_at']),  # Para buscar pr√≥ximo item            models.Index(fields=['user', 'status']),  # Para buscar requisi√ß√µes do usu√°rio            models.Index(fields=['user', 'request_type', 'status']),  # Para buscar duplicatas (otimiza find_existing_request)            # Nota: √çndice funcional para request_data->>'cnpj' ser√° criado via migration customizada        ]
An√°lise: a nota indica que um √≠ndice funcional para request_data->>'cnpj' ser√° criado, mas n√£o est√° implementado.
Recomenda√ß√£o:
# Migration customizada para criar √≠ndice GIN em JSONField# No PostgreSQL, usar √≠ndice GIN para buscas eficientes em JSONfrom django.contrib.postgres.indexes import GinIndexclass Meta:    indexes = [        # ... √≠ndices existentes ...        GinIndex(            fields=['request_data'],            name='viperrequestqueue_request_data_gin_idx',            opclasses=['jsonb_path_ops']  # Otimizado para operadores @>        ),    ]
2.4 CachedSearch - busca por nicho e localiza√ß√£o
Localiza√ß√£o: models.py:78-80
Problema:
models.pyLines 78-80
        indexes = [            models.Index(fields=['niche_normalized', 'location_normalized']),        ]
An√°lise: o √≠ndice existe, mas unique_together j√° cria um √≠ndice √∫nico. Verificar se est√° sendo usado corretamente.
Recomenda√ß√£o: manter o √≠ndice composto. Adicionar √≠ndice para last_updated se houver ordena√ß√µes frequentes:
indexes = [    models.Index(fields=['niche_normalized', 'location_normalized']),    models.Index(fields=['-last_updated']),  # Para ordena√ß√£o em listagens]
Resumo de √≠ndices recomendados
Modelo	Campo(s)	Tipo	Prioridade
LeadAccess	user, -accessed_at	Composto DESC	Alta
Lead	cached_search, -created_at (parcial CNPJ v√°lido)	Composto parcial	Alta
ViperRequestQueue	request_data	GIN (JSONB)	M√©dia
Lead	cnpj (parcial n√£o-nulo)	Parcial	M√©dia
CachedSearch	-last_updated	Simples	Baixa
3. Custo de Processamento - Campos JSON grandes
An√°lise de viper_data
3.1 Tamanho estimado do JSON
Cada viper_data pode conter:
Dados b√°sicos da empresa: ~2-5 KB
Telefones: ~1-2 KB
Emails: ~0.5-1 KB
S√≥cios/QSA: ~5-20 KB (pode ter muitos s√≥cios)
Endere√ßos: ~1-2 KB
Total estimado: 10-30 KB por lead
3.2 Impacto em mem√≥ria
Localiza√ß√£o: m√∫ltiplas fun√ß√µes em services.py
Problema:
services.pyLines 995-998
            # Sanitizar dados (esconder QSA/telefones at√© enriquecer)            sanitized_viper_data = sanitize_lead_data(                {'viper_data': lead.viper_data or {}},                show_partners=(lead_access.enriched_at is not None)            ).get('viper_data', {})
Impacto:
100 leads = 1-3 MB em mem√≥ria apenas para viper_data
Em loops, cada itera√ß√£o carrega o JSON completo
sanitize_lead_data faz copy.deepcopy(), duplicando o uso de mem√≥ria
3.3 Consultas que carregam viper_data desnecessariamente
get_leads_from_cache() - linha 1062-1065:
Carrega viper_data completo mesmo quando s√≥ precisa de campos b√°sicos
get_existing_leads_from_db() - linha 926-929:
Mesmo problema
export_leads_csv() - linha 288:
Carrega todos os viper_data mesmo que s√≥ alguns campos sejam exportados
Recomenda√ß√µes
3.1 Lazy loading de viper_data
# Carregar apenas campos necess√°rios inicialmenteleads = Lead.objects.filter(...).only(    'id', 'name', 'address', 'phone_maps', 'cnpj', 'cached_search_id')# Carregar viper_data apenas quando necess√°rio (lazy)for lead in leads:    # Acessar lead.viper_data s√≥ quando precisar sanitizar    if need_full_data:        lead.refresh_from_db(fields=['viper_data'])
3.2 Otimizar sanitize_lead_data
def sanitize_lead_data(lead_data, show_partners=False, has_enriched_access=False):    # Em vez de deepcopy, fazer c√≥pia seletiva apenas dos campos necess√°rios    sanitized = {        'name': lead_data.get('name'),        'address': lead_data.get('address'),        'phone_maps': lead_data.get('phone_maps'),        'cnpj': lead_data.get('cnpj'),    }        if 'viper_data' in lead_data and lead_data['viper_data']:        viper_data = lead_data['viper_data']        sanitized_viper = {}                # Copiar apenas campos necess√°rios (n√£o fazer deepcopy completo)        if has_enriched_access:            sanitized_viper['telefones'] = viper_data.get('telefones')            sanitized_viper['emails'] = viper_data.get('emails')            sanitized_viper['socios_qsa'] = viper_data.get('socios_qsa')                sanitized['viper_data'] = sanitized_viper        return sanitized
3.3 Usar values() para exporta√ß√£o CSV
# Em export_leads_csv, usar values() para carregar apenas campos necess√°rioslead_accesses = LeadAccess.objects.filter(    user=user_profile).select_related('lead').values(    'lead__name', 'lead__cnpj', 'lead__phone_maps',     'lead__address', 'lead__viper_data'  # Ainda precisa, mas apenas uma vez)
3.4 Considerar separa√ß√£o de dados grandes
# Criar modelo separado para dados enriquecidos (opcional, refatora√ß√£o maior)class LeadEnrichment(models.Model):    lead = models.OneToOneField(Lead, on_delete=models.CASCADE)    telefones = models.JSONField()    emails = models.JSONField()    socios_qsa = models.JSONField()    # Dados grandes separados do Lead principal
4. Estrat√©gia de Cache Global - CachedSearch
An√°lise atual
4.1 Implementa√ß√£o do CachedSearch
Localiza√ß√£o: models.py:63-83
models.pyLines 63-83
class CachedSearch(models.Model):    """    Cache global de pesquisas normalizadas para reutiliza√ß√£o.    Dados nunca expiram - base hist√≥rica permanente.    """    niche_normalized = models.CharField(max_length=255)    location_normalized = models.CharField(max_length=255)  # Formato: "Cidade - UF"    total_leads_cached = models.IntegerField(default=0)    last_updated = models.DateTimeField(auto_now=True)    expires_at = models.DateTimeField(null=True, blank=True)  # DEPRECATED: Mantido para migra√ß√£o, n√£o usado mais    created_at = models.DateTimeField(auto_now_add=True)    class Meta:        unique_together = [['niche_normalized', 'location_normalized']]        ordering = ['-last_updated']        indexes = [            models.Index(fields=['niche_normalized', 'location_normalized']),        ]
Pontos positivos:
Cache permanente (sem expira√ß√£o)
Normaliza√ß√£o de nicho e localiza√ß√£o
√çndice composto para buscas r√°pidas
4.2 Uso do cache
Localiza√ß√£o: services.py:789-813
services.pyLines 789-813
def get_cached_search(niche_normalized, location_normalized):    """    Busca um CachedSearch existente.    Dados nunca expiram - base hist√≥rica permanente.        Args:        niche_normalized: Nicho normalizado        location_normalized: Localiza√ß√£o normalizada (formato: "Cidade - UF")        Returns:        CachedSearch ou None: Cache existente ou None se n√£o existe    """    if not niche_normalized or not location_normalized:        return None        try:        cached = CachedSearch.objects.filter(            niche_normalized=niche_normalized,            location_normalized=location_normalized        ).first()                return cached    except Exception as e:        logger.error(f"Erro ao buscar cache: {e}", exc_info=True)        return None
Problemas identificados:
Sem cache em mem√≥ria (Redis/Memcached)
total_leads_cached pode ficar desatualizado
Contagem de leads √© feita com values('cnpj').distinct().count() a cada uso
4.3 Atualiza√ß√£o do cache
Localiza√ß√£o: services.py:1015-1022
services.pyLines 1015-1022
            # Contar leads √∫nicos por CNPJ usando values('cnpj').distinct()            total_leads = Lead.objects.filter(                cached_search=cached_search,                cnpj__isnull=False            ).exclude(cnpj='').values('cnpj').distinct().count()                        if cached_search.total_leads_cached != total_leads:                cached_search.total_leads_cached = total_leads                cached_search.save(update_fields=['total_leads_cached', 'last_updated'])
Problema: a contagem √© executada toda vez que h√° atualiza√ß√£o, o que pode ser custoso com muitos leads.
Recomenda√ß√µes
4.1 Adicionar cache em mem√≥ria (Redis)
# Usar Django cache framework com Redisfrom django.core.cache import cachedef get_cached_search(niche_normalized, location_normalized):    cache_key = f"cached_search:{niche_normalized}:{location_normalized}"        # Tentar cache em mem√≥ria primeiro    cached = cache.get(cache_key)    if cached:        return cached        # Se n√£o est√° em cache, buscar no banco    cached = CachedSearch.objects.filter(        niche_normalized=niche_normalized,        location_normalized=location_normalized    ).first()        if cached:        # Cachear por 1 hora        cache.set(cache_key, cached, 3600)        return cached
4.2 Otimizar atualiza√ß√£o de total_leads_cached
# Usar signal ou atualiza√ß√£o ass√≠ncronafrom django.db.models.signals import post_save, post_deletefrom django.dispatch import receiver@receiver([post_save, post_delete], sender=Lead)def update_cached_search_count(sender, instance, **kwargs):    if instance.cached_search:        # Atualizar de forma ass√≠ncrona (usar Celery ou thread)        update_cached_search_count_async.delay(instance.cached_search.id)# Ou usar contagem incremental (mais eficiente)def increment_cached_search_count(cached_search):    CachedSearch.objects.filter(id=cached_search.id).update(        total_leads_cached=models.F('total_leads_cached') + 1,        last_updated=timezone.now()    )
4.3 Adicionar estat√≠sticas de uso do cache
class CachedSearch(models.Model):    # ... campos existentes ...    hit_count = models.IntegerField(default=0)  # Quantas vezes foi usado    last_hit_at = models.DateTimeField(null=True, blank=True)        def increment_hit(self):        CachedSearch.objects.filter(id=self.id).update(            hit_count=models.F('hit_count') + 1,            last_hit_at=timezone.now()        )
4.4 Considerar particionamento para grandes volumes
# Se CachedSearch crescer muito, considerar particionamento por data# Ou usar tabela separada para estat√≠sticasclass CachedSearchStats(models.Model):    cached_search = models.OneToOneField(CachedSearch, on_delete=models.CASCADE)    total_leads = models.IntegerField()    last_counted_at = models.DateTimeField()    # Atualizar via job peri√≥dico (n√£o em tempo real)
Resumo executivo
Prioridades de otimiza√ß√£o
Alta prioridade:
Implementar .defer('viper_data') em listagens de leads
Adicionar √≠ndices parciais para CNPJs v√°lidos
Otimizar get_existing_leads_from_db para reduzir queries
M√©dia prioridade:
Implementar √≠ndice GIN para ViperRequestQueue.request_data
Adicionar cache Redis para CachedSearch
Otimizar sanitize_lead_data para evitar deepcopy completo
Baixa prioridade:
Adicionar estat√≠sticas de uso do cache
Considerar separa√ß√£o de dados grandes (refatora√ß√£o maior)
Impacto esperado
Redu√ß√£o de tempo de resposta: 40-60% em listagens de leads
Redu√ß√£o de uso de mem√≥ria: 50-70% em opera√ß√µes com muitos leads
Redu√ß√£o de carga no banco: 30-50% com √≠ndices otimizados
Melhoria em cache hits: 80-90% com Redis
M√©tricas recomendadas para monitoramento
Tempo m√©dio de resposta de queries de leads
Uso de mem√≥ria por requisi√ß√£o
Taxa de cache hit do CachedSearch
N√∫mero de queries N+1 detectadas (usar Django Debug Toolbar)
Tamanho m√©dio de viper_data por lead
]

ü§ñ 6. Instru√ß√µes para o Desenvolvedor / IA (Cursor)

Ao realizar qualquer altera√ß√£o neste reposit√≥rio, siga as diretrizes abaixo:

Prioridade de Execu√ß√£o: Siga a ordem das Fases. N√£o inicie otimiza√ß√µes de performance (Fase 4) se houver riscos de seguran√ßa abertos (Fase 1).

Integridade Financeira: Qualquer altera√ß√£o em credit_service.py deve garantir atomicidade via transaction.atomic() e prote√ß√£o contra race conditions via select_for_update().

Princ√≠pio da N√£o-Quebra: Mantenha a compatibilidade das assinaturas das fun√ß√µes at√© que a refatora√ß√£o completa da Fase 3 seja iniciada.

Verifica√ß√£o de Ownership: Toda e qualquer entrega de dados de Leads deve ser precedida por uma verifica√ß√£o na tabela LeadAccess vinculada ao user_profile da requisi√ß√£o.